"""
Main agent workflow for the ecommerce assistant
"""
import logging
import traceback
from typing import Literal
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command
from langchain_core.messages import ToolMessage, HumanMessage, SystemMessage, AIMessage

from .state import AgentState, RouterSchema
from modules.magento_tools import tools, tools_by_name
from .prompts import (
    TRIAGE_SYSTEM_PROMPT, 
    TRIAGE_USER_PROMPT,
    ASSISTANT_SYSTEM_PROMPT
)
from modules.llm.factory import get_llm_strategy
from utils.log import Logger
logger=Logger(name="agent", log_file="Logs/app.log", level=logging.DEBUG)

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.messages import SystemMessage
import os
from dotenv import load_dotenv

load_dotenv()
embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
# Load retriever from saved FAISS index
retriever = FAISS.load_local(
    "vectorstores/adobe_docs",
    embeddings,
    allow_dangerous_deserialization=True
).as_retriever(search_type="similarity", k=4)

# ---- LLM Setup ----
strategy = get_llm_strategy("openai", "")
llm = strategy.initialize()
tool_names = list(tools_by_name.keys())
print(tool_names)
print(f"total tools:{len(tool_names)}")
llm_with_tools = llm.bind_tools(tools, tool_choice="auto", parallel_tool_calls=False)
llm_router = llm.with_structured_output(RouterSchema)


def triage_router(state: AgentState) -> Command:
    """
    Route user input based on whether it should be handled or ignored.
    
    This node classifies the user input and decides whether to proceed
    with the response agent or end the conversation.
    """
    user_msg = state["user_input"]
    user_prompt = TRIAGE_USER_PROMPT.format(user_input=user_msg)
    messages = state.get("messages", [])
    result = llm_router.invoke([
        {"role": "system", "content": TRIAGE_SYSTEM_PROMPT},*messages,
        {"role": "user", "content": user_prompt},
    ])

    if result.classification == "respond":
        logger.info(f"üìß Classification: respond - {result.reasoning}")
        return Command(
            goto="response_agent",
            update={                
                "classification_decision": result.classification,
                "classification_reasoning": result.reasoning
            },
        )
    elif result.classification == "ignore":
        logger.info(f"üö´ Classification: ignore - {result.reasoning}")
        return Command(
            goto=END,
            update={"classification_decision": result.classification},
        )
    else:
        raise ValueError(f"Invalid classification: {result.classification}")     
   

def llm_call(state: AgentState):
    logger.info("üß† LLM reasoning and tool selection with RAG...")

    user_input = state["user_input"]
    messages = state["messages"]

    # Retrieve relevant docs
    relevant_docs = retriever.invoke(user_input)
    context_text = "\n\n".join(doc.page_content for doc in relevant_docs)
    #logger.debug(f"üìö Injected RAG Context:\n{context_text}")

    # Compose system prompt with embedded docs as reference
    rag_system_prompt = f"""{ASSISTANT_SYSTEM_PROMPT}

==== REFERENCE DOCUMENTATION ====
{context_text}
===============================

Use this documentation to answer the user's question if it helps.
"""

    # Call LLM with tools, injecting RAG context in system message
    response = llm_with_tools.invoke(
        [SystemMessage(content=rag_system_prompt), *messages]
    )
    if not hasattr(response, "tool_calls") or not response.tool_calls:
        logger.info("‚ÑπÔ∏è No tool calls - returning direct RAG-based answer")
        return {"messages": messages + [response]}
    # Log tool usage if any
    if hasattr(response, "tool_calls") and response.tool_calls:
        tool_names = [tc.get("name") or getattr(tc, "name", "unknown") for tc in response.tool_calls]
        logger.info(f"üîß Selected tools: {tool_names}")

    return {"messages": messages + [response]}



async def tool_handler(state: AgentState):
    """
    Execute the tools requested by the LLM.
    
    This node handles the actual execution of tools and manages
    any errors that might occur during tool execution.
    """
    logger.info("‚öôÔ∏è Executing tools...")
    last_message = state["messages"][-1]
    
    # Ensure we have an AIMessage with tool_calls
    if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
        logger.info("‚ö†Ô∏è No tool calls found in last message")
        return {"messages": state["messages"]}
    
    tool_messages = []

    for idx, tool_call in enumerate(last_message.tool_calls):
        try:
            logger.info(f"\n--- Executing Tool [{idx + 1}] ---")

            # Safe access to tool call properties
            tool_name = getattr(tool_call, "name", tool_call.get("name"))
            tool_args = getattr(tool_call, "arguments", None)
            if tool_args is None and isinstance(tool_call, dict):
                tool_args = tool_call.get("arguments") or tool_call.get("args")

            tool_id = getattr(tool_call, "id", tool_call.get("id"))

            logger.info(f"üîß Tool: {tool_name}")
            logger.info(f"üìù Args: {tool_args}")

            # Validate tool exists
            if tool_name not in tools_by_name:
                error_msg = f"Tool '{tool_name}' not found"
                logger.info(f"‚ùå {error_msg}")
                tool_messages.append(
                    ToolMessage(
                        tool_call_id=tool_id or f"unknown-{idx}",
                        content=error_msg
                    )
                )
                continue

            # Execute tool
            tool = tools_by_name[tool_name]
            result = await tool.ainvoke(tool_args)
            
            # Convert result to string if needed
            if not isinstance(result, str):
                result = str(result)
                
            logger.info(f"‚úÖ Result: {result[:100]}{'...' if len(result) > 100 else ''}")
            
            tool_messages.append(ToolMessage(tool_call_id=tool_id, content=result))

        except Exception as e:
            error_msg = f"Tool execution failed: {str(e)}"
            logger.error(f"‚ùå Error during tool_call [{idx + 1}]: {error_msg}")
            #traceback.logger.info_exc()
            tool_messages.append(
                ToolMessage(
                    tool_call_id=tool_id or f"unknown-{idx}",
                    content=error_msg
                )
            )

    return {"messages": state["messages"] + tool_messages}


def should_continue(state: AgentState) -> Literal["tool_handler", "__end__"]:
    """
    Determine whether to continue with tool execution or end the workflow.
    
    The workflow continues if there are tools to execute, and ends when
    the 'done' tool has been executed or no more tools are needed.
    """
    last_message = state["messages"][-1]
    
    # Check if it's an AIMessage with tool_calls
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        # Check if any of the tool calls is "done"
        for tool_call in last_message.tool_calls:
            tool_name = getattr(tool_call, "name", None)
            if tool_name is None and isinstance(tool_call, dict):
                tool_name = tool_call.get("name")
            if tool_name == "done":
                logger.info("üèÅ Done tool detected - will end after execution")
                return "tool_handler"  # Execute the done tool, then end
        return "tool_handler"  # Execute other tools
    
    # Check if the last message is a ToolMessage from the "done" tool
    if isinstance(last_message, ToolMessage):
        # Look for the corresponding AIMessage with tool_calls
        for i in range(len(state["messages"]) - 1, -1, -1):
            msg = state["messages"][i]
            if hasattr(msg, "tool_calls") and msg.tool_calls:
                for tool_call in msg.tool_calls:
                    tool_name = getattr(tool_call, "name", None)
                    if tool_name is None and isinstance(tool_call, dict):
                        tool_name = tool_call.get("name")
                    if tool_name == "done" and getattr(tool_call, "id", tool_call.get("id")) == last_message.tool_call_id:
                        logger.info("üèÅ Workflow complete - ending")
                        return "__end__"
                break
    
    return "__end__"


# ---- Response Agent Workflow ----
agent_workflow = StateGraph(AgentState)
agent_workflow.add_node("llm_call", llm_call)
agent_workflow.add_node("tool_handler", tool_handler)
agent_workflow.set_entry_point("llm_call")
agent_workflow.add_conditional_edges(
    "llm_call", 
    should_continue, 
    {"tool_handler": "tool_handler", "__end__": END}
)
agent_workflow.add_edge("tool_handler", "llm_call")
response_agent = agent_workflow.compile()

# ---- Overall Workflow ----
overall_workflow = (
    StateGraph(AgentState)
    .add_node("triage_router", triage_router)
    .add_node("response_agent", response_agent)
    .set_entry_point("triage_router")
    .add_conditional_edges(
        "triage_router",
        lambda state: "response_agent" if state.get("classification_decision") == "respond" else END,
        {"response_agent": "response_agent", END: END}
    )
    .compile()
)